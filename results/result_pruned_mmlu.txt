{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695236,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695236
    },
    "hendrycksTest-anatomy": {
      "acc": 0.45185185185185184,
      "acc_stderr": 0.04299268905480863,
      "acc_norm": 0.45185185185185184,
      "acc_norm_stderr": 0.04299268905480863
    },
    "hendrycksTest-astronomy": {
      "acc": 0.40131578947368424,
      "acc_stderr": 0.039889037033362836,
      "acc_norm": 0.40131578947368424,
      "acc_norm_stderr": 0.039889037033362836
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.4075471698113208,
      "acc_stderr": 0.030242233800854494,
      "acc_norm": 0.4075471698113208,
      "acc_norm_stderr": 0.030242233800854494
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4583333333333333,
      "acc_stderr": 0.04166666666666665,
      "acc_norm": 0.4583333333333333,
      "acc_norm_stderr": 0.04166666666666665
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816506,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816506
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621505,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621505
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.3699421965317919,
      "acc_stderr": 0.0368122963339432,
      "acc_norm": 0.3699421965317919,
      "acc_norm_stderr": 0.0368122963339432
    },
    "hendrycksTest-college_physics": {
      "acc": 0.20588235294117646,
      "acc_stderr": 0.04023382273617747,
      "acc_norm": 0.20588235294117646,
      "acc_norm_stderr": 0.04023382273617747
    },
    "hendrycksTest-computer_security": {
      "acc": 0.6,
      "acc_stderr": 0.04923659639173309,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.04923659639173309
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3702127659574468,
      "acc_stderr": 0.03156564682236784,
      "acc_norm": 0.3702127659574468,
      "acc_norm_stderr": 0.03156564682236784
    },
    "hendrycksTest-econometrics": {
      "acc": 0.23684210526315788,
      "acc_stderr": 0.03999423879281336,
      "acc_norm": 0.23684210526315788,
      "acc_norm_stderr": 0.03999423879281336
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3448275862068966,
      "acc_stderr": 0.039609335494512087,
      "acc_norm": 0.3448275862068966,
      "acc_norm_stderr": 0.039609335494512087
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.02141168439369419,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.02141168439369419
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.3412698412698413,
      "acc_stderr": 0.04240799327574923,
      "acc_norm": 0.3412698412698413,
      "acc_norm_stderr": 0.04240799327574923
    },
    "hendrycksTest-global_facts": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.45483870967741935,
      "acc_stderr": 0.028327743091561074,
      "acc_norm": 0.45483870967741935,
      "acc_norm_stderr": 0.028327743091561074
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.24630541871921183,
      "acc_stderr": 0.03031509928561773,
      "acc_norm": 0.24630541871921183,
      "acc_norm_stderr": 0.03031509928561773
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.49,
      "acc_stderr": 0.05024183937956911,
      "acc_norm": 0.49,
      "acc_norm_stderr": 0.05024183937956911
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.5757575757575758,
      "acc_stderr": 0.03859268142070264,
      "acc_norm": 0.5757575757575758,
      "acc_norm_stderr": 0.03859268142070264
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.46464646464646464,
      "acc_stderr": 0.03553436368828064,
      "acc_norm": 0.46464646464646464,
      "acc_norm_stderr": 0.03553436368828064
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.6217616580310881,
      "acc_stderr": 0.03499807276193339,
      "acc_norm": 0.6217616580310881,
      "acc_norm_stderr": 0.03499807276193339
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.41025641025641024,
      "acc_stderr": 0.024939313906940788,
      "acc_norm": 0.41025641025641024,
      "acc_norm_stderr": 0.024939313906940788
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.21851851851851853,
      "acc_stderr": 0.02519575225182379,
      "acc_norm": 0.21851851851851853,
      "acc_norm_stderr": 0.02519575225182379
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.42436974789915966,
      "acc_stderr": 0.03210479051015776,
      "acc_norm": 0.42436974789915966,
      "acc_norm_stderr": 0.03210479051015776
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.24503311258278146,
      "acc_stderr": 0.03511807571804725,
      "acc_norm": 0.24503311258278146,
      "acc_norm_stderr": 0.03511807571804725
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.5688073394495413,
      "acc_stderr": 0.02123336503031956,
      "acc_norm": 0.5688073394495413,
      "acc_norm_stderr": 0.02123336503031956
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.2175925925925926,
      "acc_stderr": 0.02813968944485967,
      "acc_norm": 0.2175925925925926,
      "acc_norm_stderr": 0.02813968944485967
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.5441176470588235,
      "acc_stderr": 0.034956245220154766,
      "acc_norm": 0.5441176470588235,
      "acc_norm_stderr": 0.034956245220154766
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6371308016877637,
      "acc_stderr": 0.03129920825530213,
      "acc_norm": 0.6371308016877637,
      "acc_norm_stderr": 0.03129920825530213
    },
    "hendrycksTest-human_aging": {
      "acc": 0.5201793721973094,
      "acc_stderr": 0.033530461674123,
      "acc_norm": 0.5201793721973094,
      "acc_norm_stderr": 0.033530461674123
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.44274809160305345,
      "acc_stderr": 0.04356447202665069,
      "acc_norm": 0.44274809160305345,
      "acc_norm_stderr": 0.04356447202665069
    },
    "hendrycksTest-international_law": {
      "acc": 0.6859504132231405,
      "acc_stderr": 0.04236964753041017,
      "acc_norm": 0.6859504132231405,
      "acc_norm_stderr": 0.04236964753041017
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.04803752235190193,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.04803752235190193
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.5276073619631901,
      "acc_stderr": 0.03922378290610991,
      "acc_norm": 0.5276073619631901,
      "acc_norm_stderr": 0.03922378290610991
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.38392857142857145,
      "acc_stderr": 0.04616143075028547,
      "acc_norm": 0.38392857142857145,
      "acc_norm_stderr": 0.04616143075028547
    },
    "hendrycksTest-management": {
      "acc": 0.6116504854368932,
      "acc_stderr": 0.0482572933735639,
      "acc_norm": 0.6116504854368932,
      "acc_norm_stderr": 0.0482572933735639
    },
    "hendrycksTest-marketing": {
      "acc": 0.6794871794871795,
      "acc_stderr": 0.03057281131029961,
      "acc_norm": 0.6794871794871795,
      "acc_norm_stderr": 0.03057281131029961
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.050251890762960605
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.6206896551724138,
      "acc_stderr": 0.01735126811754445,
      "acc_norm": 0.6206896551724138,
      "acc_norm_stderr": 0.01735126811754445
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.4508670520231214,
      "acc_stderr": 0.026788811931562753,
      "acc_norm": 0.4508670520231214,
      "acc_norm_stderr": 0.026788811931562753
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24022346368715083,
      "acc_stderr": 0.014288343803925293,
      "acc_norm": 0.24022346368715083,
      "acc_norm_stderr": 0.014288343803925293
    },
    "hendrycksTest-nutrition": {
      "acc": 0.434640522875817,
      "acc_stderr": 0.028384256704883037,
      "acc_norm": 0.434640522875817,
      "acc_norm_stderr": 0.028384256704883037
    },
    "hendrycksTest-philosophy": {
      "acc": 0.49517684887459806,
      "acc_stderr": 0.028396770444111298,
      "acc_norm": 0.49517684887459806,
      "acc_norm_stderr": 0.028396770444111298
    },
    "hendrycksTest-prehistory": {
      "acc": 0.4691358024691358,
      "acc_stderr": 0.027767689606833925,
      "acc_norm": 0.4691358024691358,
      "acc_norm_stderr": 0.027767689606833925
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3049645390070922,
      "acc_stderr": 0.02746470844202213,
      "acc_norm": 0.3049645390070922,
      "acc_norm_stderr": 0.02746470844202213
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3650586701434159,
      "acc_stderr": 0.012296373743443475,
      "acc_norm": 0.3650586701434159,
      "acc_norm_stderr": 0.012296373743443475
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.5110294117647058,
      "acc_stderr": 0.030365446477275675,
      "acc_norm": 0.5110294117647058,
      "acc_norm_stderr": 0.030365446477275675
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.4673202614379085,
      "acc_stderr": 0.020184583359102202,
      "acc_norm": 0.4673202614379085,
      "acc_norm_stderr": 0.020184583359102202
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5545454545454546,
      "acc_stderr": 0.047605488214603246,
      "acc_norm": 0.5545454545454546,
      "acc_norm_stderr": 0.047605488214603246
    },
    "hendrycksTest-security_studies": {
      "acc": 0.47346938775510206,
      "acc_stderr": 0.03196412734523272,
      "acc_norm": 0.47346938775510206,
      "acc_norm_stderr": 0.03196412734523272
    },
    "hendrycksTest-sociology": {
      "acc": 0.6019900497512438,
      "acc_stderr": 0.03461199429040013,
      "acc_norm": 0.6019900497512438,
      "acc_norm_stderr": 0.03461199429040013
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.71,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.71,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-virology": {
      "acc": 0.42168674698795183,
      "acc_stderr": 0.03844453181770917,
      "acc_norm": 0.42168674698795183,
      "acc_norm_stderr": 0.03844453181770917
    },
    "hendrycksTest-world_religions": {
      "acc": 0.6198830409356725,
      "acc_stderr": 0.037229657413855394,
      "acc_norm": 0.6198830409356725,
      "acc_norm_stderr": 0.037229657413855394
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "pruned-llama-experimental",
    "model_args": "pretrained=/home/zhangyihan/LMFlow/prune_log/llama-13b-mha-2-38-0.25-mlp-0.25-wikitext-512exm-1024--first,dtype=float16,use_accelerate=True",
    "num_fewshot": 5,
    "batch_size": "4",
    "batch_sizes": [],
    "device": "cuda",
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}